{"cells":[{"cell_type":"markdown","metadata":{"id":"FOgMO_1XD-fb"},"source":["# **CI Course - EX8**\n","\n","--------\n","\n","--------"]},{"cell_type":"markdown","metadata":{"id":"_GNq3wYDD-fe"},"source":["## Theory Overview ##\n","\n","What is **Artificial Neural-Networks (ANN)** ?\n","\n","Neural networks are a type of machine learning algorithm inspired by the structure and function of the human brain. They consist of interconnected nodes, or **\"neurons\"**, that process and transmit information. These networks can be trained on large amounts of data to learn patterns and relationships, and can be used for a variety of tasks such as image recognition, natural language processing, and predictive modeling.\n","\n","![Illustration_of_the_multilayer_perceptron_artificial_neural_network.png](attachment:Illustration_of_the_multilayer_perceptron_artificial_neural_network.png)\n","\n","**In a vector form:**\n","\n","\n","$$ \\left.\\begin{matrix}\n","y_1=f(\\sum_{i}^{m} w_{1i}x_1+b_1) \\\\\n","\\\\ \n","y_n=f(\\sum_{i}^{m} w_{ni}x_n+b_n)\n","\\end{matrix}\\right\\}\n","\\bar y =f(\\sum_{i}^{m} \\bar w_i\\bar x+\\bar b)\n","$$\n","\n","\n","**Where:**\n","\n","$w$ = weights \n","$$w=\\begin{bmatrix}\n"," w_{11}&...  &w_{1n} \\\\ \n"," \\vdots &  &\\vdots \\\\ \n"," w_{n1}&...  &w_{nn} \n","\\end{bmatrix}\n","$$\n","\n","$b$ = bias\n","$$\\bar b = \\begin{bmatrix} b_1, \\\\ b_2, \\\\ \\vdots\\\\ b_n\\end{bmatrix} \n","$$\n","\n","Weights and bias are a learnable variables! \n","\n","$f$ = activation function as mention in the last lecture: \n","\n","![activation_func.png](attachment:activation_func.png)\n","\n","\n","Also $x$ = inputs (or features) vector.\n","\n","$$ x = (x_1,x_2,...,x_n)^T $$\n"," \n","And $y$ = the output vector. \n","\n","$$ y = (y_1,y_2,...,y_n)^T $$\n","\n","The general process of ANN during training is **comparing the network's output $\\bar{y}$ with the desired output $y$**. **The goal** is to **minimize the difference** between them, and updating the **weights and biases** respectively, allowing us to improve the network's performance.\n","\n","The algorithm consists of two main steps: forward propagation and backward propagation."]},{"cell_type":"markdown","metadata":{"id":"TVxmCzF-D-ff"},"source":["**Forward Propagation:**\n","\n","In an artificial neural network forward pass is the calculation process where values of the output layers are obtained from the input data. It involves traversing through all neurons from the first to the last layer. \n","\n","![forward_pass.png](attachment:forward_pass.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"19lTgC4bD-ff"},"source":["**Loss function:**\n","\n","A loss function measures how good a neural network model is in performing a certain task, which in most cases is regression or classification. The value of the loss function must be minimized during the backpropagation step in order to make the neural network better.\n","\n","**Mean Absolute Error (MAE)**: Also called L1 Loss, this loss function is used for regression problems. It measures the average magnitude of errors in a set of predictions, without considering their direction1. \n","\n","$MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|$\n","\n","\n","**Mean Squared Error (MSE)**: Also called L2 Loss, this loss function is also used for regression problems. It measures the average squared difference between the predicted and actual values1. \n","\n","\n","$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$\n","\n","**Cross-Entropy Loss**: Also known as Log Loss, this loss function is used for classification problems where the output can be either 0 or 1. It measures the performance of a classification model whose output is a probability value between 0 and 11. \n","\n","$CrossEntropy =$ \n","$-\\frac{1}{n}\\sum_{i=1}^{n}[y_i\\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]$\n","\n","where $n$ is the number of samples, $y_i$ is the true value and $\\hat{y}_i$ is the predicted value."]},{"cell_type":"markdown","metadata":{"id":"oKzywK2cD-fg"},"source":["**Back-Propagation**\n","\n","The main concept of back propagating is computing the gradients of the loss function with respect to the weights and biases, **starting from the output** layer and **moving towards the input** layer.\n","\n","**General equations of B.P:**\n","\n","Let's say we have a neural network with $L$ layers, where $l \\in \\{1,...,L\\}$ denotes the layer index.<br/>The weights and biases of the network are represented by $w^l_{jk}$ and $b^l_j$, respectively.<br/>The activation of the $j^{th}$ neuron in the $l^{th}$ layer is represented by $a^l_j$.<br/>The error in the output layer $L$ is given by $\\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j)$, where $C$ is the loss (=cost) function and $\\sigma'(z)$ is the derivative of the activation function with respect to the weighted input $z$.<br/>The error in any other layer can be calculated recursively using the formula $\\delta^l_j = \\sum_k \\delta^{l+1}_k w^{l+1}_{kj} \\sigma'(z^l_j)$. <br/>The partial derivatives of the cost function with respect to the weights and biases can then be calculated as $\\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j$ and $\\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j$, respectively.\n","\n","**For a specific case:**\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"z2T6i3pAD-fg"},"source":["For example the derivative of the Mean Squared Error (MSE) loss function with respect to the predicted value $\\bar{y}$ is given by:\n","\n","$$\n","\\frac{\\partial}{\\partial \\hat{y}} MSE = \\frac{\\partial}{\\partial \\hat{y}} \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)\n","$$\n","\n","where $n$ is the number of samples and $y_i$ is the true value.\n","\n","In a neural network, the weights are updated during the backpropagation step using gradient descent. The gradient descent algorithm seeks to change the weights so that the next evaluation reduces the error. The weights are updated using the following formula:\n","\n","$$\n","w_{i new} = w_{iold} - \\alpha \\frac{\\partial L}{\\partial w_i}\n","$$\n","\n","where $w_{new}$ and $w_{old}$ are the new and old values of the weight, respectively, $\\alpha$ is the learning rate, and $\\frac{\\partial L}{\\partial w_i}$ is the partial derivative of the loss function with respect to the weight.\n","\n","In the same way we will calculate the bias update \n","\n","$$ \\bar b_{i new} = \\bar b_{iold} - \\alpha \\frac{\\partial L}{\\partial \\bar b_i }$$\n","\n","where \n","\n","$$\n","\\frac{\\partial L}{\\partial  b_i} = \\frac{\\partial L}{\\partial \\hat y} *\\frac{\\partial \\hat y}{\\partial b_i} =  \\frac{\\partial L}{\\partial \\hat y}\n","$$\n","\n","The derivative of the loss with respect to the weight depends on the specific architecture of the neural network and the chosen loss function. In general, the derivative of the loss with respect to the weight can be calculated using the chain rule. For example, let's consider a simple neural network with one input layer, one hidden layer, and one output layer. Let $x$ be the input, $w_1$ be the weight connecting the input layer to the hidden layer, $w_2$ be the weight connecting the hidden layer to the output layer, and $y$ be the output. Let $L$ be the chosen loss function. Then, the derivative of the loss with respect to $w_i$\n","$$\n","\\frac{\\partial L}{\\partial w_i} = \\frac{\\partial L}{\\partial \\hat y} *\\frac{\\partial \\hat y}{\\partial  w_i} = \\frac{\\partial L}{\\partial \\hat y} * X_i^T\n","$$\n","\n","The specific values of these partial derivatives depend on the chosen activation functions and loss function.\n","\n","\n","$$\\frac{\\partial L}{\\partial X_i}$$\n","will be the:\n","$$\\frac{\\partial L}{\\partial \\hat y}$$\n","of the previus layer \n","\n","$$\n","\\frac{\\partial L}{\\partial  X_i} = \\frac{\\partial L}{\\partial \\hat y}*\\frac{\\partial \\hat y}{\\partial X_i} = \\frac{\\partial L}{\\partial \\hat y}*W_i^T\n","$$\n","\n","-------\n"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"-PC5h1C5D-fg"},"source":["### Exercise - Predicting fuel consumption ###\n","\n","The purpose of the exercise is to train a neural network to predict fuel consumption (in km/l) based on various features of cars. The nueral network will be built from scratch! \n","\n","**Input and Output:**\n","\n","The **input** of the code is a dataset containing **399 data samples** of **4 features of cars**, such as displacement, number of cylinders, horsepower, and weight. The **output** is the **fuel consumption** (km/l) corresponding to each sample (**=regression problem**).\n","\n","To enhance the definition of the network architecture, we will employ object-oriented programming (OOP) techniques by implementing multiple classes.\n","\n","**Classes:**\n","\n","1. `FCLayer`: This class represents a fully connected layer in the neural network.\n","\n","2. `ActivationLayer`: This class implements the activation function for the neural network. It uses the hyperbolic tangent (tanh) activation function and its derivative.\n","\n","![tanh.png](attachment:84f30145-c854-4090-806b-7d2c71c2f9f5.png)\n","\n","The derivative of the activation tanh is: \n","\n","$$\\frac{d}{dx} \\tanh(x) = \\frac{d}{dx} \\left( \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\right) = 1 - \\tanh^2(x)\n","$$\n","\n","\n","3. `Network`: This class represents the neural network itself.\n","\n","**Solution Steps:**\n","\n","1. Define and initialize the neural network classes\n","\n","2. Load and preprocess the data\n","\n","3. Creating the network\n","\n","4. Training!\n","\n","5. Evaluate\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"hl_4gWeUD-fh"},"source":["#### 0. Import libraries ####\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A5SvTW76D-fh"},"outputs":[],"source":["import numpy as np\n","import pickle\n","from sklearn.preprocessing import StandardScaler\n","import matplotlib.pyplot as plt\n","%matplotlib qt\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{"id":"W7Tw-20nD-fi"},"source":["#### 1. Define and initialize the neural network classes ####\n"]},{"cell_type":"markdown","metadata":{"id":"1Mm0imc1D-fj"},"source":["FCLayer:\n","- Purpose: This class represents a fully connected layer in the neural network, responsible for forward and backward propagation.\n","- Actions: Initialization of the layer with random weights and biases, forward propagation calculation, and backward propagation updates."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oyfeYiwsD-fj"},"outputs":[],"source":["class FCLayer():\n","    # input_size = number of input neurons\n","    # output_size = number of output neurons\n","    def __init__(self, input_size, output_size):\n","        self.input = None\n","        self.output = None\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.weights = np.random.rand(output_size, input_size) - 0.5\n","        self.bias = np.random.rand(1, output_size,) - 0.5\n","\n","    # returns output for a given input\n","    def forward_propagation(self, input_data):\n","        self.input = input_data\n","        self.output = np.dot(self.weights, self.input) + self.bias\n","        return self.output.reshape(-1,)\n","\n","    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n","    #### NOTE: for the code we will use dE as dL from the explanetion \n","    def backward_propagation(self, output_error, learning_rate):\n","        input_error = np.dot(self.weights.T, output_error) # dE/dx\n","        weights_error = np.dot(output_error.reshape(-1,1), self.input.reshape(1,-1)) #dE/dw\n","\n","        # update parameters\n","        self.weights -= learning_rate * weights_error\n","        self.bias -= learning_rate * output_error # as we saw in the explanation output eror = dE/db \n","        return input_error"]},{"cell_type":"markdown","metadata":{"id":"suw0kJBWD-fj"},"source":["ActivationLayer:\n","- Purpose: This class implements the activation function for the neural network.\n","- Actions: Initialization of the layer, including the tanh activation function and its derivative, forward propagation calculation, and backward propagation calculation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"guLRs_lPD-fj"},"outputs":[],"source":["class ActivationLayer():\n","    def __init__(self):\n","        self.input = None\n","        self.output = None\n","        self.activation = lambda x: np.tanh(x)\n","        self.activation_prime = lambda x: 1-np.tanh(x)**2\n","\n","    # returns the activated input\n","    def forward_propagation(self, input_data):\n","        self.input = input_data\n","        self.output = self.activation(self.input)\n","        return self.output\n","\n","    # Returns input_error=dE/dX for a given output_error=dE/dY.\n","    # No \"learnable\" parameters ->  no learning rate\n","    def backward_propagation(self, output_error, learning_rate):\n","        return self.activation_prime(self.input) * output_error"]},{"cell_type":"markdown","metadata":{"id":"4Yv9vgYTD-fk"},"source":["Network:\n","- Purpose: This class represents the neural network and provides methods for adding layers, defining the loss function, predicting output, and training the network.\n","- Actions: Initialization of the network, addition of layers, definition of the **mean squared error loss** function, prediction of output, and training the network using backpropagation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tBYg2rMFD-fk"},"outputs":[],"source":["class Network:\n","    def __init__(self):\n","        self.layers = []\n","        self.loss = None\n","        self.loss_prime = None\n","\n","    # add layer to network\n","    def add(self, layer):\n","        self.layers.append(layer)\n","\n","    # Loss function \n","    def mse(self, y_true, y_pred):\n","        return np.mean(np.power(y_true-y_pred, 2)) # np.power is faster\n","    \n","    # Loss function and its derivative\n","    def mse_prime(self, y_true, y_pred):\n","        return 2*(y_pred-y_true)/y_true.size\n","\n","    # predict output for given input\n","    def predict(self, input_data):\n","        # sample dimension first\n","        samples = len(input_data)\n","        result = []\n","\n","        # run network over all samples\n","        for i in range(samples):\n","            # forward propagation\n","            output = input_data[i]\n","            for layer in self.layers:\n","                output = layer.forward_propagation(output)\n","            result.append(output)\n","\n","        return result\n","\n","    # train the network\n","    def fit(self, x_train, y_train, epochs, learning_rate):\n","        # sample dimension first\n","        samples = len(x_train)\n","\n","        # training loop\n","        E = []\n","        for i in range(epochs):\n","            err = 0\n","            for j in range(samples):\n","                # forward propagation\n","                output = x_train[j]\n","                for layer in self.layers:\n","                    output = layer.forward_propagation(output)\n","\n","                # compute loss (for display purpose only)\n","                err += self.mse(y_train[j], output)\n","\n","                # backward propagation\n","                error_grad = self.mse_prime(y_train[j], output)\n","                for layer in reversed(self.layers):\n","                    error_grad = layer.backward_propagation(error_grad, learning_rate)\n","\n","            # calculate average error on all samples\n","            err /= samples\n","            print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n","            E.append(err)\n","\n","            plt.figure(0)\n","            plt.cla()\n","            plt.plot(range(len(E)), E)\n","            plt.xlabel('Epochs')\n","            plt.ylabel('Error')\n","            plt.pause(0.000001)"]},{"cell_type":"markdown","metadata":{"id":"xtSamaC_D-fk"},"source":["#### 2. Load and preprocess the data ####"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yeGbi49LD-fk","outputId":"669d8bf2-7617-4db9-f79e-839767887f8f"},"outputs":[{"name":"stdout","output_type":"stream","text":["example of X[0]: [   8.  307.  130. 3504.] | example of Y[0]: [3.401152]\n","X shape is: (399, 4) | Y shape is: (399, 1)\n"]}],"source":["# training data\n","with open('auto_kml.pkl', 'rb') as H:\n","    data = pickle.load(H)\n","\n","# Input: Features of various cars\n","# 0. displacement\n","# 1. Number of cylinders\n","# 2. horsepower\n","# 3. weight\n","# Output: Fuel consumption (km/l)\n","\n","X = data['features']\n","Y = data['kml'].reshape(-1,1)\n","\n","print(f\"example of X[0]: {X[0]} | example of Y[0]: {Y[0]}\")\n","print(f\"X shape is: {X.shape} | Y shape is: {Y.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lWgdZ8lTD-fl"},"outputs":[],"source":["D = np.concatenate((X,Y), axis=1)\n","\n","## normalizing for better learning process of the net\n","scaler = StandardScaler()\n","scaler.fit(D)\n","D = scaler.transform(D)\n","\n","## splitting the data for test and train\n","\n","X_train, X_test, y_train, y_test = train_test_split(D[:,:-1], D[:,-1], test_size=0.15, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"c1BouSGvD-fl"},"source":["#### 3. Creating the network ####\n","\n","Our network architecture will consist of **4 connected layers**, with the **tanh activation function applied after each layer**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XX4U8jQeD-fl"},"outputs":[],"source":["# Create network\n","net = Network()\n","#layer1 + act1\n","net.add(FCLayer(X_train.shape[1], 3))\n","net.add(ActivationLayer())\n","#layer2 + act2\n","net.add(FCLayer(3, 3))\n","net.add(ActivationLayer())\n","# #layer3 + act3\n","net.add(FCLayer(3, 3))\n","net.add(ActivationLayer())\n","# #layer4 + act4\n","net.add(FCLayer(3, 1))\n","net.add(ActivationLayer())"]},{"cell_type":"code","source":["## Hyperparameters\n","epochs = 80\n","learning_rate = 0.2"],"metadata":{"id":"V0FjyulqIWVB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_FQM4K_SD-fm"},"source":["#### 4. Training ####"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"87nQ6dGHD-fm","outputId":"7c2c28d4-a0e6-419e-a254-d4ccf788477a"},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch 1/80   error=0.239973\n","epoch 2/80   error=0.086333\n","epoch 3/80   error=0.076210\n","epoch 4/80   error=0.073244\n","epoch 5/80   error=0.071816\n","epoch 6/80   error=0.070984\n","epoch 7/80   error=0.070445\n","epoch 8/80   error=0.070068\n","epoch 9/80   error=0.069790\n","epoch 10/80   error=0.069574\n","epoch 11/80   error=0.069402\n","epoch 12/80   error=0.069260\n","epoch 13/80   error=0.069140\n","epoch 14/80   error=0.069037\n","epoch 15/80   error=0.068948\n","epoch 16/80   error=0.068869\n","epoch 17/80   error=0.068798\n","epoch 18/80   error=0.068734\n","epoch 19/80   error=0.068676\n","epoch 20/80   error=0.068623\n","epoch 21/80   error=0.068574\n","epoch 22/80   error=0.068529\n","epoch 23/80   error=0.068487\n","epoch 24/80   error=0.068448\n","epoch 25/80   error=0.068412\n","epoch 26/80   error=0.068377\n","epoch 27/80   error=0.068345\n","epoch 28/80   error=0.068314\n","epoch 29/80   error=0.068284\n","epoch 30/80   error=0.068256\n","epoch 31/80   error=0.068230\n","epoch 32/80   error=0.068204\n","epoch 33/80   error=0.068179\n","epoch 34/80   error=0.068155\n","epoch 35/80   error=0.068132\n","epoch 36/80   error=0.068110\n","epoch 37/80   error=0.068088\n","epoch 38/80   error=0.068067\n","epoch 39/80   error=0.068046\n","epoch 40/80   error=0.068026\n","epoch 41/80   error=0.068007\n","epoch 42/80   error=0.067988\n","epoch 43/80   error=0.067969\n","epoch 44/80   error=0.067951\n","epoch 45/80   error=0.067933\n","epoch 46/80   error=0.067916\n","epoch 47/80   error=0.067899\n","epoch 48/80   error=0.067882\n","epoch 49/80   error=0.067865\n","epoch 50/80   error=0.067849\n","epoch 51/80   error=0.067834\n","epoch 52/80   error=0.067818\n","epoch 53/80   error=0.067803\n","epoch 54/80   error=0.067788\n","epoch 55/80   error=0.067774\n","epoch 56/80   error=0.067760\n","epoch 57/80   error=0.067746\n","epoch 58/80   error=0.067732\n","epoch 59/80   error=0.067719\n","epoch 60/80   error=0.067705\n","epoch 61/80   error=0.067693\n","epoch 62/80   error=0.067680\n","epoch 63/80   error=0.067668\n","epoch 64/80   error=0.067656\n","epoch 65/80   error=0.067644\n","epoch 66/80   error=0.067633\n","epoch 67/80   error=0.067621\n","epoch 68/80   error=0.067610\n","epoch 69/80   error=0.067600\n","epoch 70/80   error=0.067589\n","epoch 71/80   error=0.067579\n","epoch 72/80   error=0.067569\n","epoch 73/80   error=0.067559\n","epoch 74/80   error=0.067550\n","epoch 75/80   error=0.067540\n","epoch 76/80   error=0.067531\n","epoch 77/80   error=0.067522\n","epoch 78/80   error=0.067513\n","epoch 79/80   error=0.067505\n","epoch 80/80   error=0.067496\n","\n","Train loss:  0.06747583678422203\n"]}],"source":["net.fit(X_train, y_train, epochs=epochs, learning_rate=learning_rate)\n","print()\n","predictions = net.predict(X_train)\n","err = 0\n","for y_p, y in zip(predictions, y_train):\n","    err += net.mse(y, y_p)\n","print('Train loss: ', err/len(y_train))"]},{"cell_type":"markdown","metadata":{"id":"wzdrzG7AD-fm"},"source":["#### 5. Evaluate ####"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g8GBK1BMD-fm","outputId":"459cb15e-6077-4db5-b967-ccbec5071830"},"outputs":[{"name":"stdout","output_type":"stream","text":["Given input X_test[0]: [-0.85446201 -0.99101366 -0.88884502 -1.21835308]\n","Prediction is: -0.8547638371622026 | Real is: -0.8544620067201791\n","______\n","Total Test loss:  0.06805227332219639\n"]}],"source":["predictions = net.predict(X_test)\n","print(f\"Given input X_test[0]: {X_test[0]}\")\n","print(f\"Prediction is: {predictions[0][0]} | Real is: {y_test[0]}\")\n","\n","print(\"______\")\n","\n","err = 0\n","for y_p, y in zip(predictions, y_test):\n","    err += net.mse(y, y_p)\n","print('Total Test loss: ', err/len(y_test))"]},{"cell_type":"markdown","metadata":{"id":"hXX-zFlWD-fm"},"source":["## Summary ##\n","\n","In this class we covered:\n","\n","1. ANN structure and main concepts.\n","2. Learning process of ANN.\n","3. Build an ANN from scratch and use it to solve regression problem of car fuel consumption predicting based on its features.\n","\n","---"]},{"cell_type":"markdown","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"aKa3p60bD-fn"},"source":["## Helpful and extra links ##\n","\n","1. [Neural Network In 5 Minutes](https://www.youtube.com/watch?v=bfmFfD2RIcg)\n","2. [Neural Networks: Inside the Black Box](https://www.youtube.com/watch?v=CqOfi41LfDw)\n","3. [Neural Networks: Forward pass and Backpropagation](https://towardsdatascience.com/neural-networks-forward-pass-and-backpropagation-be3b75a1cfcc)\n","4. [Back-Propagation algorithm: A step by step demonstration](https://www.youtube.com/watch?v=YOlOLxrMUOw)\n","---"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}